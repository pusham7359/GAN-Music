# -*- coding: utf-8 -*-
"""
Created on Wed Oct 17 11:25:23 2018

@author: Bhargav
"""

# -*- coding: utf-8 -*-
"""GAN_MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hsSYowYuIS0xxMdfJaH8ToTsiGGs4Iye
"""

#Generative Adversarial Networks with MNIST

import numpy as np
from scipy.io import loadmat
import keras
import midi_manipulation,glob
from tqdm import tqdm
from random import randint
import keras.backend as K
from keras.layers import Dense, Activation, LeakyReLU, BatchNormalization,Conv2D, Conv2DTranspose
from keras.layers import Dropout, Reshape, Flatten,Input
from keras.models import Sequential, Model
from keras.optimizers import Adam
from keras import initializers
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from midiutil import MIDIFile
import random
from scipy.io import loadmat
# %matplotlib inline

"""## Loading Beethoven Dataset"""

def get_songs(path):
    files = glob.glob('{}/*.mid*'.format(path))
    songs = []
    for f in tqdm(files):
        try:
            song = np.array(midi_manipulation.midiToNoteStateMatrix(f))
            if np.array(song).shape[0] > 50:
                songs.append(song)
        except Exception as e:
            raise e           
    return songs

songs = get_songs('Beethoven') #These songs have already been converted from midi to msgpack
print ("{} songs processed".format(len(songs)))
print(len(songs))
maxlen = 0 
minlen = 10000
for i in range(len(songs)):
    maxlen = max(songs[i].shape[0],maxlen)
    minlen = min(songs[i].shape[0],minlen)

for i in range(len(songs)):
    songs[i] = songs[i][:minlen,:]
    songs[i] = np.asarray(songs[i]).reshape(minlen*174)

noiseList = []
for i in range(minlen):
    noise = np.zeros(174)
    noise[randint(0,173)] = 1
    noiseList.append(noise)


X_noise = np.asarray(noiseList).reshape(minlen*174)
X_train = np.asarray(songs)
#X_noise = X_noise.reshape(maxlen*174)

# Plot the loss from each batch
def plotLoss(epoch):
    plt.figure(figsize=(10, 8))
    plt.plot(dLosses, label='Discriminitive loss')
    plt.plot(gLosses, label='Generative loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig('gan_images/gan_loss_epoch_%d.png' % epoch)
    
# Create a wall of generated MNIST images
def plotGeneratedImages(epoch, examples=100, dim=(10, 10), figsize=(10, 10)):
    noise = np.random.normal(0, 1, size=[examples, 100])
    generatedImages = generator.predict(noise)
    generatedImages = generatedImages.reshape(examples, 28, 28)

    plt.figure(figsize=figsize)
    for i in range(generatedImages.shape[0]):
        plt.subplot(dim[0], dim[1], i+1)
        plt.imshow(generatedImages[i], interpolation='nearest', cmap='gray_r')
        plt.axis('off')
    plt.tight_layout()
    plt.savefig('gan_images/gan_generated_image_epoch_%d.png' % epoch)

def saveModels(epoch):
  generator.save('gan_models/gan_generator_epoch_%d.h5' % epoch)
  discriminator.save('gan_models/gan_discriminator_epoch_%d.h5' % epoch)

# Optimizer
adam = Adam(lr=0.0002, beta_1=0.5)

generator = Sequential()
generator.add(Dense(2000, input_dim=minlen, kernel_initializer=initializers.RandomNormal(stddev=0.02)))
generator.add(LeakyReLU(0.2))
generator.add(Dense(3000))
generator.add(LeakyReLU(0.2))
generator.add(Dense(4500))
generator.add(LeakyReLU(0.2))
generator.add(Dense(28014, activation='tanh'))
generator.compile(loss='binary_crossentropy', optimizer=adam)

discriminator = Sequential()
discriminator.add(Dense(28014, input_dim=(minlen*174), kernel_initializer=initializers.RandomNormal(stddev=0.02)))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dropout(0.3))
discriminator.add(Dense(4500))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dropout(0.3))
discriminator.add(Dense(200))
discriminator.add(LeakyReLU(0.2))
discriminator.add(Dropout(0.3))
discriminator.add(Dense(1, activation='sigmoid'))
discriminator.compile(loss='binary_crossentropy', optimizer=adam)

# Combined GAN network
discriminator.trainable = False
ganInput = Input(shape=(minlen,))
x = generator(ganInput)
ganOutput = discriminator(x)
gan = Model(inputs=ganInput, outputs=ganOutput)
gan.compile(loss='binary_crossentropy', optimizer=adam)

dLosses = []
gLosses = []

def train(epochs=1, batchSize=128):
    batchCount = int(X_train.shape[1] / batchSize)
    print ('Epochs:', epochs)
    print ('Batch size:', batchSize)
    print ('Batches per epoch:', batchCount)

    for e in range(1, epochs+1):
        for  i in range(batchCount):
            # Get a random set of input noise and images
            noise = np.random.normal(0, 1, size=[batchSize, minlen])
            imageBatch = X_train[np.random.randint(0, X_train.shape[0], size=batchSize)]
            print(i)
            # Generate fake MNIST images
            generatedImages = generator.predict(noise)
            print(generatedImages.shape)
            #midi_manipulation.noteStateMatrixToMidi(generatedImages,i, span=(108-21))
            # print np.shape(imageBatch), np.shape(generatedImages)
            X = np.concatenate([imageBatch, generatedImages])

            # Labels for generated and real data
            yDis = np.zeros(2*batchSize)
            # One-sided label smoothing
            yDis[:batchSize] = 0.9

            # Train discriminator
            discriminator.trainable = True
            dloss = discriminator.train_on_batch(X, yDis)

            # Train generator
            noise = np.random.normal(0, 1, size=[batchSize, minlen])
            yGen = np.ones(batchSize)
            discriminator.trainable = False
           

if __name__ == '__main__':
    train(1, 128)

